{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB+paperQA\n",
    "\n",
    "这是一个用于从PDF或文本文件（可以是原始HTML）进行问题回答的最简化软件包。它力求给出非常好的答案，不产生幻觉，通过用文本中的引用来支持回答。\n",
    "\n",
    "默认情况下，它使用OpenAI Embeddings与一个称为FAISS的向量数据库来嵌入和搜索文档。然而，通过langchain，您可以使用开源模型或嵌入（详见下文）。\n",
    "\n",
    "PaperQA使用下面显示的过程：\n",
    "\n",
    "1. 将文档嵌入到向量中\n",
    "2. 将查询嵌入到向量中\n",
    "3. 在文档中搜索前k个片段\n",
    "4. 创建与查询相关的每个片段的摘要\n",
    "5. 将摘要放入提示中\n",
    "6. 用提示生成答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating a persistent Chroma client\n",
    "client = chromadb.PersistentClient(path=\"client_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(name=collection_test_4_literature)]\n"
     ]
    }
   ],
   "source": [
    "# 查看所有 Collation\n",
    "collections_list = client.list_collections()\n",
    "print(collections_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建、检查和删除集合\n",
    "#collection = client.create_collection(name=\"collection_test_4_literature\")\n",
    "# collection = client.get_collection(name=\"collection_test_4_literature\")\n",
    "\n",
    "# 删除 collection\n",
    "client.delete_collection(\"collection_test_4_literature\")\n",
    "\n",
    "# 重新创建\n",
    "collection = client.create_collection(name=\"collection_test_4_literature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_all_files_and_names_in_subfolders(directory):\n",
    "    # 获得文件下所有文件的绝对路径\n",
    "    dic = {}\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)  # 获取文件的绝对路径\n",
    "            file = os.path.splitext(file)[0]\n",
    "            dic[file] = file_path\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 获取所有文件路径\n",
    "file_name_path_dic = get_all_files_and_names_in_subfolders(\"literature_base/知识库文献集\")\n",
    "\n",
    "# 文献的具体数据表格\n",
    "db_detail = file_name_path_dic.pop('db_detail')\n",
    "df_01 = pd.read_excel(db_detail,sheet_name=0,index_col=0)\n",
    "df_02 = pd.read_excel(db_detail,sheet_name=1,index_col=0)\n",
    "\n",
    "\n",
    "# 文献的摘要和id\n",
    "doc_lis = []\n",
    "id_lis = []\n",
    "\n",
    "meta_data_lis =[]\n",
    "for i in range(len(df_01.index)):\n",
    "    doc_lis.append(df_01['Abstract'][i+1])\n",
    "    id_lis.append(df_01['id '][i+1])\n",
    "    meta_data = {'name' : df_01['name'][i+1] ,\n",
    "            'Publication Date' : df_01['Publication Date'][i+1] \n",
    "        }\n",
    "    meta_data_lis.append(meta_data)\n",
    "\n",
    "\n",
    "for i in range(len(df_02.index)):\n",
    "    doc_lis.append(df_02['Abstract'][i+1])\n",
    "    id_lis.append(df_02['id '][i+1]\n",
    "                  )\n",
    "    meta_data = {'name' : df_02['name'][i+1],\n",
    "            'Publication Date' : df_02['Publication Date'][i+1] \n",
    "        }\n",
    "    meta_data_lis.append(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文件摘要和idf、metadata放入集合\n",
    "collection.add(\n",
    "    documents=doc_lis,\n",
    "    ids=id_lis,\n",
    "    metadatas = meta_data_lis\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询示例\n",
    "a = collection.query(\n",
    "    query_texts=['什么是纳米微气泡'],\n",
    "    n_results=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SL02-02-J', 'SL02-01-A']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.get(ids=[\"id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SL01-01-J': 'literature_base/知识库文献集\\\\SL01\\\\SL01-01-J.pdf',\n",
       " 'SL01-02-J': 'literature_base/知识库文献集\\\\SL01\\\\SL01-02-J.pdf',\n",
       " 'SL02-01-A': 'literature_base/知识库文献集\\\\SL02\\\\SL02-01-A.pdf',\n",
       " 'SL02-02-J': 'literature_base/知识库文献集\\\\SL02\\\\SL02-02-J.pdf'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_path_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从db中查询文档，并且返回最相关的文献路径\n",
    "\n",
    "def retriever_db(question,n_results=2):\n",
    "    a = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=n_results,\n",
    "        )\n",
    "    \n",
    "    id_lis = a['ids'][0]\n",
    "    \n",
    "    path_lis = []\n",
    "    for _id in id_lis:\n",
    "        _path = file_name_path_dic[_id]\n",
    "        path_lis.append(_path)\n",
    "    \n",
    "    return path_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化paperQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "# OPENAI_API_BASE\n",
    "#OPENAI_API_BASE: \"https://api.openai-forward.com/v1\"\n",
    "#OPENAI_PROXY: \"http://localhost:33210\"\n",
    "os.environ['OPENAI_API_BASE'] = \"https://api.openai-forward.com/v1\"\n",
    "os.environ['OPENAI_PROXY'] = \"http://localhost:33210\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa import Docs\n",
    "import chromadb\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_all_files_and_names_in_subfolders(directory):\n",
    "    # 获得文件下所有文件的绝对路径\n",
    "    dic = {}\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)  # 获取文件的绝对路径\n",
    "            file = os.path.splitext(file)[0]\n",
    "            dic[file] = file_path\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class paperQA_bot():\n",
    "    def __init__(self) -> None:\n",
    "        # Initiating a persistent Chroma client\n",
    "        self.client = chromadb.PersistentClient(path=\"client_test\")\n",
    "        self.collection = self.client.get_collection(name=\"collection_test_4_literature\")\n",
    "    \n",
    "    def run(self,question):\n",
    "        docs = self.init_docs(question)\n",
    "        self.docs = docs\n",
    "        answer = docs.query(question)\n",
    "        return answer.formatted_answer\n",
    "    \n",
    "    def Continue_asking(self,question):\n",
    "        return self.docs.query(question).formatted_answer\n",
    "    \n",
    "    def init_docs(self,question):\n",
    "        # 初始化QA文档\n",
    "        # 检索文档\n",
    "        my_docs = self.retriever_db(question=question)\n",
    "        # 将文档加入paperQA的文档库中\n",
    "        #docs = Docs() # 默认使用4.0\n",
    "        docs = Docs(llm='gpt-3.5-turbo')\n",
    "        for d in my_docs:\n",
    "            docs.add(d)\n",
    "        return docs\n",
    "    \n",
    "    # 从db中查询文档，并且返回最相关的文献路径\n",
    "    def retriever_db(self,question,n_results=2)->list :\n",
    "        file_name_path_dic = get_all_files_and_names_in_subfolders(\"literature_base/知识库文献集\")\n",
    "        \n",
    "        a = self.collection.query(\n",
    "            query_texts=[question],\n",
    "            n_results=n_results,\n",
    "            )\n",
    "        id_lis = a['ids'][0]\n",
    "        path_lis = []\n",
    "        for _id in id_lis:\n",
    "            _path = file_name_path_dic[_id]\n",
    "            path_lis.append(_path)\n",
    "        \n",
    "        return path_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = paperQA_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = bot.run('什么是QSAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: QSAR模型的适用域是什么\\n\\nQSAR模型的适用域是指模型在哪些情况下能够有效地进行预测和分类。适用域的确定对于模型的性能至关重要，特别是在监管应用中。文献提到，描述符系统和适用域表征方法之间的兼容性对于分类器的性能非常重要。例如，非三维Mordred描述符基于的分类器在平均AUC超过0.86的情况下表现良好，与先前报告的分类器相当。然而，基于ECFP4指纹的分类器在某些化合物上表现较差，这是由于训练中存在偏差。相比之下，MACCS键显示出比ECFP4指纹更好的泛化能力。因此，适用域的不当组合可能导致分类器的意外性能下降。(Wang2020 pages 4-4)\\n\\nReferences\\n\\n1. (Wang2020 pages 4-4): Wang, Zhongyu, et al. \"Applicability Domains Enhance Application of PPARγ Agonist Classifiers Trained by Drug-like Compounds to Environmental Chemicals.\" Chem. Res. Toxicol., vol. 33, no. 6, 2020, pp. 1382-1388.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.Continue_asking('QSAR模型的适用域是什么')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('langchainbot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa5caed4deed2d7d5dcd6765afb0c1931f123425f69d6026e7a35865929c7b69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
